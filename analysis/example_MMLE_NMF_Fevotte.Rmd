---
title: "Marginal Maximum Likelihood Estimator - NMF"
output:
  html_notebook: default
  pdf_document: default
---
[Marginal Maximum Likelihood Estimator - NMF (MMLE)](http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6232464) tries to learn a dictionary $W$ of a Non-Negative matrix Factorization that maximizes
\begin{align}
\underset{\mathbf{W}}{\operatorname{argmax}}  \qquad p(\boldsymbol{V} | \boldsymbol{W}) = \int_{\boldsymbol{H}} p(\boldsymbol{V} | \boldsymbol{W}\boldsymbol{H})p(\boldsymbol{H}) d\boldsymbol{H}
\label{eq:mmle}
\end{align}

Unlike classic NMF, here we place a prior over the coefficients matrix $\mathbf{H}$ and the integrate it out. It is been reported in the original paper that it has the nice property of setting columns of $\mathbf{W}$ to 0 when we try to fit for latent dimensions $\mathbf{K}$ than necessary, which makes it an interesting way of regularizing.

In this notebook I show different examples of MMLE-NMF using different inference methods, namely:
  
  * Variational Inference
  * MC-EM
  * MCMC-EM
  
### Data
  
We will first load a toy matrix $\mathbf{V}$ of dimensions $F \times N$ that has been procuded from two $\mathbf{W}, \mathbf{H}$ matrices with $K$ latent dimensions. In our example, $K=5, F=10, N=50$.
  
```{r}

library(dplyr)
library(tidyr)
library(reshape2)
library(ggplot2)

# Load NMIST data
# V <- load_image_file("../data/train-images.idx3-ubyte")
# V <- V[, 1:10]
# V <- V - min(V) # make it positive!

data <- load_synthetic()
V <- data$V
F <- dim(V)[1]
N <- dim(V)[2]

plot_multiple_W(data$V, data$W, data$H, cols=3)
```

### Classic NMF with multiplicative updates

A classic way of doing NMF is trying to minimize the Kullbackâ€“Leibler divergence (the I-divergence, indeed, which is a slightly adapted KL) between the original data and the recontruction achieved by the decomposition:
\begin{align}
\underset{\mathbf{W, H}}{\operatorname{argmin}} \qquad D_{KL}(\mathbf{V} | \mathbf{WH})
\end{align}

In [Lee and Seung, 2000](https://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf) the authors proved that we can minimize the KL divergence by iteratively updating the two matrices:

\begin{align}
h_{a\mu}^{t+1}
=
h_{a\mu}^{t}
\frac{
\sum_i
v_{i\mu} 
w_{ia} / (\mathbf{W} \mathbf{H})_{i\mu}
}
{\sum_i w_{ia}}
\qquad
w_{ia}^{t+1}
=
w_{ia}^{t}
\frac{
\sum_\mu
v_{i\mu} 
h_{a\mu} / (\mathbf{W} \mathbf{H})_{i\mu}
}
{\sum_v h_{av}}
\end{align}

If we run the algorithm for $K=5$ we obtain this (sometimes it might fall in some local optima):

```{r}
example_LeeSeung <- function(V, K){
  # Random initialization
  initH <- matrix(runif(K*N), nrow = K, ncol= N)
  initW <- matrix(runif(F*K), nrow = F, ncol = K)
  
  # NMF
  res <- nmf_Lee(V, K, initW, initH, maxiters = 1000)
  
  plot_multiple_W(res$W%*%res$H, res$W, res$H, cols=3)
}

```

```{r}
example_LeeSeung(data$V, 5)
```


The two suggested matrices $\mathbf{W}$ and $\mathbf{H}$ are, as we see in the Figure, a very good decomposition of $\mathbf{V}$. An interesting observation is that these are not the original $\mathbf{W}$ and $\mathbf{H}$: this are similar to the original ones under some permutation of the $K$ labels!. This is a similar phenomena to what we find in Mixture Models, where we have K! symmetries in the posterior, where $K$ is the number of components.

In order to motivate the next models, let see what happens if we try to factorize our data using too many latent dimensions $K$ (more than the number of dimensions in our original $\mathbf{W}$ and $\mathbf{H}$ matrices)

```{r}
example_LeeSeung(data$V, 10)
```

When $K$ is too high then some of the extra dimensions take also some of the responsability, even if in some cases it is very low.

Let see what happens with the MMLE-NMF model.

### MMLE-NMF with Variational Inference
Let's see how the Variational Bayes EM performs for different choices of $K$.

```{r}
K <- 5
res5vb <- nmf_mmle_vbem(data$V, 5, maxiters = 20)
res10vb <- nmf_mmle_vbem(data$V, 10, maxiters = 20)
res15vb <- nmf_mmle_vbem(data$V, 15, maxiters = 20)

plot_multiple_W(data$W, res5vb$W, res10vb$W, res15vb$W, cols=4)
```

As promised in the paper, we see that values in the exceeding rows of the dictionary tend to zero.

### MMLE-NMF with MCEM

We will know use the algorithm MC-EM algorithm proposed in the MMLE-NMF paper to estimate $\mathbf{W}$.

```{r}
K <- 5 
initW <- matrix(1, nrow = F, ncol = K)
res5mcem <- nmf_mmle_mcem(V, K, initW, maxiters = 50, ngibbs = 25, mc = "mcem")

K <- 10
initW <- matrix(1, nrow = F, ncol = K)
res10mcem <- nmf_mmle_mcem(V, K, initW, maxiters = 50, ngibbs = 25, mc="mcem")

K <- 15
initW <- matrix(1, nrow = F, ncol = K)
res15mcem <- nmf_mmle_mcem(V, K, initW, maxiters = 50, ngibbs = 25, mc = "mcem")
```

```{r}
plot_multiple_W(data$W, res5mcem$W, res10mcem$W, res15mcem$W, cols=4)
```

Again, as promised in the paper, we see that values in the exceeding rows of the dictionary tend to zero.

### MMLE-NMF with SAEM

The Stochastic Approximation Expectation-Maximization (SAEM) updates the $Q$ objective function in the following way:
\begin{align}
Q^{t+1} = 
(1 - \gamma_k)Q_{k-1}(\theta) + \lambda_k
\left(
\frac{1}{m(k)} \sum_{j=1}^{m(k)} \ln p(\mathbf{Z}^{r}(j), \mathbf{X} | \theta)
\right)
\end{align}

```{r}
K <- 5
initW <- matrix(1, nrow = F, ncol = K)
res5saem <- nmf_mmle_mcem(V, K, initW, maxiters = 50, ngibbs = 25, mc = "saem")

K <- 10
initW <- matrix(1, nrow = F, ncol = K)
res10saem <- nmf_mmle_mcem(V, K, initW, maxiters = 50, ngibbs = 25, mc = "saem")

K <- 15
initW <- matrix(1, nrow = F, ncol = K)
res15saem <- nmf_mmle_mcem(V, K, initW, maxiters = 50, ngibbs = 25, mc = "saem")
```

```{r}
plot_multiple_W(data$W, res5saem$W, res10saem$W, res15saem$W, cols=4)
```


### Benchmark of inference methods for MMLE-KL

We now compare the likelihood $p(\mathbf{V} | \mathbf{W})$ attained by the different inference methods. We will use Chib's method for the MC algorithms (see the original paper) and the lower bound for the vartional.

```{r}
# In the paper they use:
# maxiters = 4000
# ngibbs = 100 samples / iteration
K <- 5
Kmax <- 10
likes_mcem <- rep(NA, Kmax)
likes_saem <- rep(NA, Kmax)
likes_vb <- rep(NA, Kmax)

for(K in 1:Kmax){
  cat('\n K=', K)
  initW <- matrix(1, nrow = F, ncol = K)
  
  res <- nmf_mmle_mcem(data$V, K, initW, maxiters = 50, ngibbs = 25, mc = "mcem")
  likes_mcem[K] <- marginal_likelihood_Chib(res$V, res$W, res$H[,,dim(res$H)[3]], res$C)
  
  res <- nmf_mmle_mcem(data$V, K, initW, maxiters = 50, ngibbs = 25, mc = "saem")
  likes_saem[K] <- marginal_likelihood_Chib(res$V, res$W, res$H[,,dim(res$H)[3]], res$C)
} 

for(K in 1:Kmax){
  cat('\n K=', K)
  res <- nmf_mmle_vbem(data$V, K, maxiters = 50)
  likes_vb[K] <- max(res$lower_bound)
}
```

```{r}
df.likes <- data.frame(mcem = likes_mcem, saem = likes_saem, vb = likes_vb, K=1:length(likes_mcem)) %>%
            gather('algorithm', 'likelihood', -K)

ggplot(df.likes, aes(x = K, y = likelihood, group=algorithm, 
                     color = algorithm, linetype=algorithm, shape=algorithm)) + 
  geom_line() + geom_point() + theme_bw()
```







